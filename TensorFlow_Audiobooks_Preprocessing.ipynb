{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example of Audiobooks store "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess the data. Balance the dataset. Create 3 datasets:training, validation, and test. Save the newly created sets in a tensor friendly format(e.g*.npz)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Since we are dealing with real life data, we will need to preprocess it a bit. This is the relevant code, which is not that hard, but refers to data engineering more than machine learning.\n",
    "If you want to know how to do that, go throgh code and comments. In any case, this should do the trick for all datasets organized in the way: many inputs, and then 1 cell containing the targets (all supervized learning datasets).\n",
    "\n",
    "Note that we have removed the header row, which contains the name of the categories. we simply want the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract the data from the csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "\n",
    "raw_data = np.loadtxt('Audiobooks.csv',delimiter=',')\n",
    "unscaled_inputs_all = raw_data[:,1:-1]\n",
    "targets_all = raw_data[:,-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Balance the dataset"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1. We will count the number of targets that are 1s\n",
    "2. We will keep as many 0s as 1s(we will delete the others)\n",
    "3. lets count the all target that are 1s if we sum all the targets we can get the number of targets that are 1s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_one_targets = int(np.sum(targets_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2237"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_one_targets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note: we have 2237 1s and we will keep that much in 0s in the targets and remove rest of the targets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_targets_counter=0\n",
    "indices_to_remove = []\n",
    "\n",
    "for i in range(targets_all.shape[0]):\n",
    "    # The shape of targets_all on axis = 0, is basically the length of the vector\n",
    "    # in the loop we want to increase the zeroes counter by 1, if the target is 0\n",
    "    if targets_all[i] == 0:\n",
    "        zero_targets_counter = zero_targets_counter + 1\n",
    "        # if the target at position i 0, and the number of zeroes is bigger than the number of 1's, we want to take note of that index\n",
    "        if zero_targets_counter>num_one_targets:\n",
    "            indices_to_remove.append(i)\n",
    "              # if the target at position i is 0, and the number of zeroes is the bigger than the number of 1s, I'll know the indices of all data points to be removed.\n",
    "                \n",
    "        \n",
    "unscaled_inputs_equals_priors = np.delete(unscaled_inputs_all, indices_to_remove, axis=0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis=0)  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Now if we check all 1's and 0's both will be same and that means we have balanced data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2237\n",
      "2237\n"
     ]
    }
   ],
   "source": [
    "num_one_targets = int(np.sum(targets_equal_priors))\n",
    "print(num_one_targets)\n",
    "print(targets_equal_priors.shape[0]-num_one_targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Standardize the inputs"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Standardize the data means it will greatly imporve our algorithms performance.\n",
    "We will use the preprocessing libaray we imported from sklearn\n",
    "preprocessing.scale(X) is a method that standardizes an array along an axis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equals_priors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffle the data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A little trick is to shuffle the inputs and the targets. We keep the same information but in random order.\n",
    "since we will be batching, we actually must shuffle the data.\n",
    "Batching: There will be many batch in a dataset we can batch per data one batch.\n",
    "Inside a batch the data is homogeneous, but between batches it is very hetrogeneous.\n",
    "\n",
    "np.arrange([start], stop) is a method that returns a evenly spaced values withing a given interval.\n",
    "Then we use\n",
    "np.random.shuffle(X) is a method that shuffles the numbers in a given sequence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_inputs = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the dataset into train, validation, and test"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "I'll use the 80-10-10 split for train,validation, and test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-160.00124748257753 3579 -0.04470557347934549\n",
      "21.10995037610438 447 0.04722583976757132\n",
      "138.89129710376912 448 0.31002521674948463\n"
     ]
    }
   ],
   "source": [
    "samples_count = shuffled_inputs.shape[0]\n",
    "\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validation_samples_count = int(0.1*samples_count)\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "#We have the size of the train, validation, and test. Let's extract them.\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_inputs[:train_samples_count]\n",
    "\n",
    "validation_inputs = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets = shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "\n",
    "test_inputs = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets = shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "\n",
    "# It is useful to check if we have balanced the dataset\n",
    "# The all should be arround 50%\n",
    "\n",
    "\n",
    "print(np.sum(train_targets), train_samples_count, np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets), validation_samples_count, np.sum(validation_targets) / validation_samples_count)\n",
    "print(np.sum(test_targets), test_samples_count, np.sum(test_targets) / test_samples_count)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "It seems all three sets are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the three datasets in *.npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('Audiobooks_data_train', inputs = train_inputs, targets = train_targets)\n",
    "np.savez('Audiobooks_data_validation', inputs = validation_inputs, targets = validation_targets)\n",
    "np.savez('Audiobooks_data_test', inputs = test_inputs, targets = test_targets)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Note : Each time we run the code in this sheet, we will preprocess the data once again (forgetting the preprocessing)\n",
    "You can use the same code to preprocess any dataset that has two classes\n",
    "Now afterward we use npz files for machine learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
